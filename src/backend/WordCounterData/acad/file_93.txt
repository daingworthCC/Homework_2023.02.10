4013841 ABSTRACT : The belief that the success of an information system( IS) is proportional to the extent of user participation in developing the system has become axiomatic in IS research. Despite this widely held conviction, prior research in participation strategy has yielded mixed results. The purpose of this research is to incorporate the theoretical framework of procedural justice into the user participation paradigm to clarify salient psychological factors influencing the relationship between participation and specific outcomes. <p> In this experiment, 207 student subjects took part in improving payroll data entry software. Subjects were promised three increasing levels of participation crossed with two levels of participation expectations( e.g., either promised and actual participation were congruent or promised participation exceeded actual participation). A control group was used where subjects were neither promised nor provided any participation. When promised and actual participation were congruent, higher levels of participation resulted in corresponding increases in user attitudes( procedural justice, control, satisfaction, and task commitment) and performance. Conversely, when promised participation exceeded actual participation, increased participation levels led to monotonic decreases in user @ @ @ @ @ @ @ @ @ @ can be either functional or dysfunctional. The strong attitudinal and behavioral findings of this study complement and extend procedural justice theories, and study results encourage the integration of procedural justice concepts into IS user participation research. <p> Key Words : Accounting information systems, Procedural justice, Participation, Expectations. <p> Data Availability : Please contact the author. <p> In spite of many attempts to characterize the effect of user participation in information systems( IS) development on user attitudes and behavior, empirical findings have been mixed( Barki and Hartwick 1989, 1994 ; Lawrence and Low 1993 ; Pettingell et al. 1989 ; Ives and Olson 1984). Barki and Hartwick( 1989,1994) and Wong-On-Wing( 1988) suggested that these inconsistent results may arise from of weaknesses in conceptual development, theoretical support and methodological rigor. Motivated by a desire to strengthen these weaknesses and to explicate the equivocal findings of prior participation research, this study integrates the theoretical framework of procedural justice into the user participation paradigm in an effort to more clearly understand @ @ @ @ @ @ @ @ @ @ development process. <p> Procedural justice refers to the perceived fairness of the decision-making process( Thibaut and Walker 1975). The focus is on the process itself as opposed to the decision outcome. Procedural justice theories provide a way to clarify the relationship between participation and outcomes by examining critical process variables as well as methods of operationalizing these variables. Section I briefly describes the current state of IS research in participation strategy. A discussion of procedural justice theories along with presentation of hypotheses follows in section II. Section III describes the experimental method. The results are presented in section IV. and section V discusses the findings. I. INFORMATION SYSTEMS PARTICIPATION RESEARCH <p> Some IS researchers suggest that user participation in the development( i.e., purchase, design, modification or implementation) of an IS is integral to the success of the system( Cushing 1990 : Ives and Olson 1984 ; Martin 1982, 1984 ; Senn 1978). Given the decision to use a participation strategy, the extent of participation in developing an IS can @ @ @ @ @ @ @ @ @ @ over all aspects of the development process. Ives and Olson( 1984) suggested that stronger participation strategies may have greater consequential payoffs, such as enhanced levels of user attitudes and performance. <p> The impact of vesting higher levels of control in IS users has been described by Mumford et al.( 1983), Mumford( 1981) and Mumford and Henshall( 1979), who identified three levels of IS user participation :( 1) consultative,( 2) representative, and( 3) consensus. Consultative participation gives users the lowest level of control, as the developer makes the decisions while considering the input of IS users. A higher level of control is present in representative participation where IS personnel and users jointly develop the system. User control is maximized in consensus participation where all IS users participate continuously in designing the system and IS personnel, and users have equal voting power in all development issues. Underlying the participation levels just described is the issue of user control. Higher levels of user participation @ @ @ @ @ @ @ @ @ @ behaviors. <p> Control is also a major factor in user-led development, in which either users or their designated representatives have control over all aspects of the development project( Lodge 1989). Lawrence and Low( 1993) found that the most significant factors leading to IS user satisfaction were the extent of control and representation perceived by users during the development process. That is, when users believe their values, concerns and opinions are fairly represented throughout the decision-making process, satisfaction increases correspondingly. <p> Doll and Torkzadeh( 1988,1989,1991) examined end-user participation in developing specific applications. Doll and Torkzadeh( 1988) identified five components of end-user computing satisfaction :( 1) content,( 2) accuracy,( 3) format,( 4) ease-of-use, and( 5) timeliness. In a theoretical model of end-user participation in IS development, Doll and Torkzadeh( 1989) contended that the degree of discrepancy between end-users ' actual and desired participation influences perceptions of satisfaction. Empirical results supported the basic premise that when desired @ @ @ @ @ @ @ @ @ @ IS satisfaction is negative. Doll and Torkzadeh( 1991) subsequently found that the degree of congruence between desired and actual participation was a better predictor of end-user computing satisfaction than were perceptions of participation. <p> Joshi( 1989) developed an instrument primarily designed to measure equity issues in an IS setting, based on equity theory. A small portion of Joshi 's( 1989) instrument dealt with fairness of the means of allocating IS resources, based on procedural justice. Validity of the instrument was tested by surveying IS users of information products and services provided by a centralized IS function. Joshi( 1990) subsequently found that overall user satisfaction was positively related to favorable perceptions of fair and equitable IS resource allocations. <p> Recent research in procedural justice offers promise in helping to clarify inconsistencies across participation studies by offering a firm theoretical basis for understanding and operationalizing participation strategies. While procedural justice researchers also predict a positive relationship between participation and outcomes, their focus is on the decision-making process as opposed to the decision outcome. @ @ @ @ @ @ @ @ @ @ procedural justice reflects the extent to which a decision process is perceived to be fair or in accordance with accepted norms of behavior( Thibaut and Walker 1975, 1978). Perceptions of procedural justice are an affective feeling about the fairness of decision making. These feelings are believed to exert significant influence on the attitudes and behaviors of affected parties( Leventhal 1980). Instrumental Voice and Choice <p> A consistent finding in the procedural justice literature is that when those affected by a decision are allowed the opportunity to express their views and these views become manifest in the decision outcome, procedural justice judgments are enhanced( Lind and Tyler 1988). The control-oriented theory of procedural justice attributes these robust findings to the degree of influence or control the affected parties believe they have over the decision outcome( Leventhal 1980 ; Thibaut and Walker 1975). The positive relationship observed between soliciting the opinions, concerns and preferences of affected parties and perceived procedural justice is termed the instrumental voice effect( Folger 1977). Participation by voice increases @ @ @ @ @ @ @ @ @ @ control when the affected party believes the expression of an opinion might lead to favorable outcomes( Houlden et al. 1978 ; Folger 1977). Instrumental voice can be viewed as probabilistic control because the precise degree of influence is uncertain. <p> When affected parties are allowed participation via selection of one option from many alternatives, this is referred to as choice effect( Earley and Lind 1987). Participation by choice increases perceptions of process fairness since choice is a form of decision control. Choice can be viewed as deterministic control because the degree of influence is certain. As with voice, choice also is congruent with the control-oriented perspective of procedural justice because choice is instrumental in influencing the decision outcome. <p> One unresolved question in procedural justice research is whether the control level manifest in choice negates the need to provide instrumental voice to affected parties. That is, does instrumental voice plus choice represent a stronger form of participation than choice alone? Greenberg( 1990) suggested that voice and choice represent unique psychological properties. @ @ @ @ @ @ @ @ @ @ most common dependent measures examined in procedural justice studies are :( 1) procedural justice perceptions,( 2) control perceptions,( 3) task commitment,( 4) outcome satisfaction, and( 5) performance. Procedural justice judgments concern affected parties ' perception of how fair or just were the procedures used throughout the decision making. Perceptions of control reflect the degree of influence affected parties believed they had over the decision making and/or the decision outcome. Task commitment indicates the motivation level held by affected parties to perform the task at hand. The affected party 's degree of satisfaction with the ultimate decision is mirrored in the outcome satisfaction measure. Performance is measured via a behavioral response, generally by performing tasks related to the decision process and outcome. Empirical evidence generally supports the proposition that participation by instrumental voice enhances the attitudes and behavior of affected parties, and participation by choice produces even stronger effects( Lind and Tyler 1988 ; Earley and Lind 1987 ; Locke 1976). <p> The voice and choice @ @ @ @ @ @ @ @ @ @ Developing an IS is an organizational activity involving a delicate balance between meeting IS users ' requirements and allocating scarce organizational resources( Martin 1984). Due to practical constraints( e.g., technical, financial, operational and control), it is unlikely that all IS users will receive everything they expect and demand from the IS. Inevitably, trade-offs must be made. The issues of constraints and trade-offs call for employing the appropriate participation strategy, via the judicious use of voice and/or choice, for the situation. Therefore, the following hypothesis is proposed : <p> H1 : Increasing levels of participation via instrumental voice, choice, and instrumental voice plus choice will lead to corresponding increases in user attitudes and performance. Noninstrumental Voice <p> Some procedural justice researchers believe that voice can enhance perceptions of procedural justice for reasons other than influence or control( Lind and Tyler 1988 ; Lerner 1981). This is called the value-expressive perspective of procedural justice and is manifest in noninstrumental voice. <p> The value-expressive perspective asserts that participation by voice @ @ @ @ @ @ @ @ @ @ sense of inclusiveness and self-esteem, thereby promoting perceptions of being treated fairly even when voice has no instrumental influence on the decision outcome. Noninstrumental voice is important because it has been linked to more global organizational variables, such as increased loyalty to the organization, enhanced trust in the supervisor, reduced absenteeism, and increased intention to stay( Lind and Tyler 1988). <p> Participation by noninstrumental voice tends to increase perceptions of procedural justice, but other variables( e.g., perceived control, outcome satisfaction, task commitment and performance) are not influenced because noninstrumental voice does not change the decision outcome( Lane 1988). Earley and Lind( 1987) found support for the noninstrumental voice effect. They found that adding noninstrumental voice to choice resulted in higher recorded levels of procedural fairness as compared to choice alone. Causal modeling analysis revealed that judgments of procedural justice were not mediated by control, providing additional support to the value-expressive perspective of procedural justice. <p> Noninstrumental voice can be applied to an IS development setting. @ @ @ @ @ @ @ @ @ @ decision is made. User input might be solicited regarding the ease of use of accounting software after the software has been selected and implemented. This input may be used to evaluate but not necessarily alter the software. As a consequence, voice would be viewed as noninstrumental, and one would expect little impact on attitudes and behavior with the exception of procedural justice. Accordingly, hypothesis two is : <p> H2 : The addition of noninstrumental voice to instrumental voice plus choice will increase perceptions of procedural justice, but other attitudes and performance will be unaffected. The Frustration Effect <p> The general patterns referred to above have been reversed unexpectedly in a few procedural justice studies : in the face of ostensibly fairer procedures, corresponding attitudes and performance decreased. This phenomenon has been labeled " the frustration effect "( Folger 1977). Subsequent studies by Folger et al.( 1979,1983), and Cohen( 1985) have documented similar results. The frustration effect seems to occur when participants feel their input into the decision making was @ @ @ @ @ @ @ @ @ @, expectations concerning the nature and extent of participation were not met. <p> Cohen( 1985) believed this phenomenon can occur in allocation decision environments where the allocator has a vested interest in the allocation. Here, the decision maker gives the guise of procedural fairness( a smoke screen) rather than a genuine attempt to involve participants in the decision process. As a consequence, frustration is likely to emerge due to a violation of the affected parties ' participation expectations. In a field experiment, Baldwin et al.( 1991) documented the frustration effect described by Cohen( 1985). <p> Lind et al.( 1990) also discussed possible adverse consequences of deceptive exercises of user involvement when the objective is only to promote the appearance of participation. They conjectured that such abuses of voice and choice may not be uncommon in organizational settings. Lind and Tyler( 1988) speculated that the frustration effect, where perceived participation does not meet expectations, may be operating unknowingly in some participation studies, thereby contributing @ @ @ @ @ @ @ @ @ @ participation expectations are met may be an important moderating factor influencing perceptions of procedural justice and associated outcomes. <p> The issue of violating a priori expectations can be applied to the IS development setting. Doll and Torkzadeh( 1991) reported that incongruent states between desired and actual participation can have deleterious effects on attitudes and performance. The frustration effect is not unlike Doll and Torkzadeh 's( 1991) congruence construct, as they both involve violations of a priori conditional states. With the frustration effect, expectations rather than desires are violated. One would expect frustration levels to escalate with promises and subsequent violations of increased levels of user participation. In turn, the control-oriented perspective of procedural justice would predict that as expectations of increased participation levels are violated, corresponding reductions in feelings of control or influence should follow. Therefore, hypothesis three is : <p> H3 : When participation expectations are violated, increasing levels of participation in defining user interface requirements via instrumental voice, choice and instrumental voice plus choice will lead to corresponding decreases in @ @ @ @ @ @ @ @ @ @ is promised and expectations surrounding that promise are subsequently violated, it is expected that only perceptions of procedural justice will be negatively influenced due to feelings of decreased inclusion, violated trust and lowered self-esteem. As previously explained, other salient attitudes and behavior should be unaffected. This follows the value-expressive perspective of procedural justice. Therefore, hypothesis four is : <p> H4 : When participation expectations are violated, the addition of noninstrumental voice to instrumental voice plus choice will decrease perceptions of procedural justice, but other attitudes and performance will be unaffected. <p> The hypotheses are depicted graphically in figure 1. III. METHOD Subjects <p> Participants, students at a large state university, were recruited from introductory sections of behavioral science and principles of management, required courses for all business majors. A total of 216 participants took part in an experiment for research credit. Nine subjects were dropped from the study because they did not understand the manipulations. The resulting usable sample size was 207. The mean( standard deviation) age of the @ @ @ @ @ @ @ @ @ @ the gender distribution was 98 males and 109( 53 percent) females. The number of subjects in the seven treatment conditions averaged 30 with a high of 31 and a low of 28. Design <p> A 3( mode-of-participation) X 2( **25;473;TOOLONG) crossed factorial design was used. There was an additional control group( mute condition) where expectations of participation were neither promised nor received. There were three increasing mode-of-participation levels : instrumental voice, instrumental voice plus choice, and instrumental voice plus choice plus noninstrumental voice. **26;500;TOOLONG were either confirmed or violated. <p> The experiment was conducted over 27 sessions lasting about 1.5 hours each. Due to the nature of the manipulations, only one treatment condition could be run at each session. Thus, each experimental condition was run three times with experimental times randomly assigned to conditions. To avoid possible experimenter demand effects, the experiment was conducted double-blind, where neither the subjects nor experimenter were aware of the hypotheses. Procedure <p> Subjects who believed they were color-blind were asked @ @ @ @ @ @ @ @ @ @ designed to lead the subjects to select certain colors. 1 Subjects reported to a computer laboratory in small groups of 5 to 12. The room contained 15 networked computer terminals and a master terminal. Each computer was surrounded by a partition so that no subject could view the computer screen of another subject, subjects wore earplugs to block the noise of other keyboards in the room and subjects were not allowed to talk during the experiment. Subjects were randomly assigned to treatment conditions and randomly seated in the laboratory. Subjects in all but the mute condition were told the experimenter was interested in how they might improve the initial payroll screen to make it easier to use. <p> Subjects were informed there would be three data entry periods : a three-minute training period, a five-minute practice period and a ten-minute performance period. The purpose of the training period was to familiarize the subjects with the computer keyboard. Subjects then entered payroll data into a black and white payroll screen( called the practice-screen) for the five-minute practice period. <p> @ @ @ @ @ @ @ @ @ @ that state lottery tickets would be offered as an incentive during the performance period. Subjects were also told that their performance during both the three-minute training period and the five-minute practice period would be used as a handicap for the ten-minute performance period-in effect, giving everyone in the room an equal chance to earn state lottery tickets. <p> Subjects in the mute condition, who were neither promised nor granted any participation in modifying the practice screen, were presented with an improved performance screen, and the ten-minute performance period commenced. For subjects in the remaining treatment conditions, the presentation of the performance screen and commencement of the ten-minute performance period occurred immediately after the voice and choice manipulations. <p> After the ten-minute performance period, all subjects answered three categories of questions : dependent variable, manipulation check and general. To preclude a possible order effect, the questions were randomized within each category for each subject. All responses and performance measures were captured on computer diskette. Finally, subjects were debriefed and paid two state lottery tickets for @ @ @ @ @ @ @ @ @ @ beginning of the experiment, all subjects in the instrumental voice condition were promised they would be asked to express their preferences for certain payroll screen attributes. After the practice period, subjects were asked to express their personal preferences for five possible payroll screen modifications. The first three proposed changes were cosmetic in nature, and the last two were functional changes, as follows : Background color : The background color of the practice-screen was black. Subjects expressed a preference for either a( a) blue or( b) green background. Data entry field color : The field color of the practice-screen was white. Subjects expressed a preference for either( a) magenta or( b) yellow fields. Placement of the current date field : The current date field( a static field) displayed the current date on the top center of the practice screen. Subjects expressed a preference to place this field on either the( a) top right or( b) top left side of the payroll screen. " Are you @ @ @ @ @ @ @ @ @ @ appeared after entry of each data item on the practice screen allowing the subject to correct an erroneous entry. Subjects expressed a preference to either( a) present this prompt after entering all data for an entire timecard or( b) eliminate the prompt. Entry of timecard dates : To input timecard dates into the practice screen, subjects were required to enter each date( Monday through Sunday) for each timecard. Subjects expressed a preference to either( a) enter Monday 's date only and have the computer fill in Tuesday through Sunday dates or( b) enter the payroll date range( Monday through Sunday) before entering any timecards and have the computer automatically assign those dates to each timecard. <p> After the subjects expressed their preferences, the experimenter explained that, due to the particular combination of attributes chosen and due to software, time and network constraints, no one in this session would be able to receive their functional preferences. The experimenter assured the subjects that every effort had been made to grant @ @ @ @ @ @ @ @ @ @, field color and placement of the current date field). <p> The experiment was designed to provide all subjects in all treatment conditions with essentially the same improved performance screen having a blue background and magenta data entry fields. The current date field would be placed where the subject preferred in the **25;528;TOOLONG confirmed condition and opposite of the subject 's preference in the **25;555;TOOLONG violated condition. 2 No functional changes were made to the payroll screen to facilitate comparison of satisfaction and performance measures between treatment conditions. <p> In the **25;582;TOOLONG confirmed condition, subjects were led into preferring the blue background and magenta fields using false feedback regarding an opinion survey of professional data entry clerks. Granting the subjects three of their five preferences was intended to provide them with a sense of instrumentality. In the **25;609;TOOLONG violated condition, the same false feedback method was used to lead the subjects into preferring a green background and yellow data entry fields. Instrumental voice was violated in that the cosmetic attributes of the performance screen were opposite of the subjects ' expectations @ @ @ @ @ @ @ @ @ @ voice participation mode was manipulated as previously described. After subjects expressed their preferences, the experimenter presented subjects with three alternative payroll screens, based on their personal preferences, from which they could choose one. Subjects were told their chosen alternative would appear on the upcoming performance screen. Only one alternative contained all three expressed cosmetic preferences. In the **25;636;TOOLONG confirmed condition, subjects received their choice and in the **25;663;TOOLONG violated condition, subjects did not receive their chosen option. Manipulation of Instrumental Voice Plus Choice Plus Noninstrumental Voice <p> The instrumental voice and choice modalites and **25;690;TOOLONG conditions were manipulated as previously described. After the ten-minute performance period, subjects were promised that they would be given an opportunity to evaluate the strengths and weaknesses of the improved the payroll screen( i.e., noninstrumental voice). Subjects were told that these final recommendations would be considered for evaluation purposes. <p> In the **25;717;TOOLONG confirmed condition, subjects were provided with a word-processing screen and they had several minutes to write any additional comments and recommendations. In the @ @ @ @ @ @ @ @ @ @ they would not be allowed to express these final comments and recommendations. No justification for violating this promise was provided. IV. RESULTS Pilot Testing <p> Four pilot tests were conducted prior to the actual experiment. The purpose of the first pilot was to test the experimental materials, particularly the software developed for the experiment, and to examine subject receptivity to the manipulations. Suggestions from participants were integrated into the experimental materials and manipulations. <p> A second pilot test was conducted to obtain preliminary attitudinal and behavioral results. A possible scale-bound problem was detected using seven-point attitudinal measures. The researcher decided to try a nine-point scale to provide more latitude to respondents, as used by Lind et al.( 1990). Also, performance was relatively flat across conditions and subjects indicated that an incentive program might help. <p> The third pilot test revealed that the nine-point scale appeared to work better than the seven-point scale, as mean responses appeared to differentiate themselves across the spectrum of manipulations. Students were told they could earn from @ @ @ @ @ @ @ @ @ @ performance differences were noticed between groups, but subjects indicated that the top award of $2 was not enough incentive because the keying trial was quite tedious. Subjects told the experimenter that one dollar-state lottery tickets would be more valuable to them than the equivalent amount of money. Hence, in the fourth pilot test a : state lottery ticket incentive program was tested where students could earn either one or two lottery tickets. This incentive program appeared to work better than using money in that distinguishable performance differences were noted between the treatment conditions. Measures <p> Performance, measured during the ten-minute performance period, was the net correct number of data entry characters( i.e., total number of data entry characters minus the number of incorrect data entry characters) entered by each participant. Attitudinal measures included nine-point, bi-polar attitudinal scales measuring perceptions of( 1) procedural justice,( 2) perceived control,( 3) overall satisfaction with the performance screen, and( 4) task commitment. Two items were included for each construct. @ @ @ @ @ @ @ @ @ @ were adapted from Earley and Lind( 1987). Overall satisfaction items were adapted from Doll and Torkzadeh( 1988) and Joshi( 1990). Related construct items were averaged to form single indices, whose correlations were.89 for procedural justice,.92 for perceived control,.71 for overall IS satisfaction, and.86 for task commitment. Panel A of the appendix, presents the dependent variable questions used in this study. <p> Manipulation check questions assessed whether the subject perceived the promise( 1 = not promised ; 9 = promised) of the instrumental voice, choice and noninstrumental voice modalities. A second set of three questions assessed whether the subject perceived the receipt( 1 = not received : 9 = received) of these participation modalities. The absolute difference between the promised and received means were calculated to assess the success of the **25;771;TOOLONG manipulation. Manipulation Checks <p> Wording of the manipulation-check questions appears in the the appendix, panel B. Oneway ANOVA models with seven levels( the mute condition plus the six manipulated conditions @ @ @ @ @ @ @ @ @ @ An additional three ANOVA models were run on the absolute differences between the promised and received means for the instrumental voice, choice and noninstrumental voice modalities. All nine ANOVA models were significant at p =.01. Duncan 's Multiple Range tests were next calculated( alpha =.01) for each ANOVA model to determine significant mean differences between treatment conditions. All manipulation-check means were in the anticipated direction and significantly different where expected. Therefore, manipulations of the independent variables were considered successful. <p> Question 13( appendix, panel C) was administered to subjects in the violated conditions to determine why they believed the performance-screen did not contain their preferences and/ or choices. Of the 89 subjects in the violated treatment conditions, two subjects did not provide an answer to this question. The answers provided by 87 subjects were categorized as either( a) experimenter did not care about my preferences and/or choices( n = 39), or( b) the researcher just asked me to express my preferences and/or choices to appease me @ @ @ @ @ @ @ @ @ @ into the performance screen( n = 48). This indicates that subjects likely attributed the violated expectations on the failure of the experimenter to genuinely consider the their input during the decision-making process. Under this condition, Cohen( 1985) and Baldwin et al.( 1991) warned that the frustration effect is most likely to emerge. <p> As a check on the attribution of subject frustration, question 8( appendix, panel C) was asked of all subjects and question 12( appendix, panel C) was administered only to subjects in the violated treatment conditions. Question 8 attempted to determine whether subjects in the violation conditions were more or less frustrated with the experimenter than subjects whose expectations were not violated. If so, the attitudinal and behavioral effects observed in the violated conditions may have been partially a result of subjects ' frustration attribution toward the experimenter personally, rather than toward( un) fair decision making( Wong-On-Wing 1988). From an ANOVA for all seven treatment conditions for question 8, the @ @ @ @ @ @ @ @ @ @ =.21). There were no significant differences between treatment groups, and the relatively low mean indicated that subjects were not frustrated with the experimenter. From an ANOVA for the three **25;798;TOOLONG violated conditions for question 12, the mean response was 8.14( F =.32, p =.72). There were no significant differences between treatment conditions, but the relatively high mean indicated that subjects were frustrated because their performance-screen did not include their desired attributes. The results of questions 8, 12 and 13 presented above indicate that subjects in the violated treatment conditions likely experienced the frustration effect phenomenon( Lind and Tyler 1988 ; Cohen 1985). Pre-Hypothesis Testing Subject Differences <p> Questions 1, 2 and 3( appendix, panel C) are general post-test questions on which subject differences were analyzed. The average subject age was 22.71, and there were 98 male and 109 female participants. The ANOVA F-ratio( p-values) were.41(.71) for age and 1.48(. 19) for gender. Subjects also self-reported @ @ @ @ @ @ @ @ @ @. The mean response was 5.73 and the F-ratio( p-value) was.32(.92). No significant differences were detected between groups on these three variables. Based on the results, study findings were likely not systematically influenced by subject differences examined. Tests for Interactions <p> Each dependent measure initially was tested separately in a 3( mode-of-participation) X 2( **25;825;TOOLONG) design. ANOVA procedures were used in analyzing the attitudinal questions and ANCOVA was used to analyze performance using results of the three-minute training period and five-minute practice period as covariates. Results of the ANOVA/ANCOVA factor effects model for each dependent measure are shown in table 1, panel A. <p> Interactions were significant( p </-.01) for each dependent measure. To interpret these effects, a cell means model was calculated for each dependent measure and the results are shown in table 1, panel B. The overall p-values for the treatment conditions were significant( p </-.01) for all cell means models. The response measures for the mute condition were included in the @ @ @ @ @ @ @ @ @ @ levels. The Bartlett test for homogeneity( alpha =.05) indicated that the variance of the responses in the mute condition was not significantly different from the average variance of the corresponding responses in the 3 X 2 ANOVA/ANCOVA models. Since the variances were not significantly different and because the subjects in the mute condition were randomized into that condition as part of the overall experiment, the mute condition responses were included in the one-way ANOVA/ANCOVA models( Neter et al. 1990). Experiment- Wise Error Rate <p> To alleviate concerns over experiment-wise error rate, two precautions were taken. First, a Bonferroni-type adjustment was calculated over the five separate ANOVA/ANCOVA models( one for each dependent variable). This calculation revealed that since each separate ANOVA/ANCOVA model yielded an overall significance level of p </-.0001, the experiment-wise alpha level was about.0005( 1-( 1-.0001 5)). Second, to control the overall error rate within each ANOVA/ANCOVA model, Duncan 's Multiple Range tests were used for pairwise comparisons in all hypothesis testing. @ @ @ @ @ @ @ @ @ @.72( procedural justice and performance) to r =.89( perceived control and task commitment). These correlations were significant at p =.01, as shown in table 2. Dependent measures were not collapsed because Lind and Tyler( 1988), after examining numerous procedural justice studies, concluded that the covariation of these measures is a result of causal ordering, not lack of independence. Specifically, perceptions of control often positively influence perceptions of procedural justice, which, in turn, lead to corresponding perceptions of outcome satisfaction and task commitment. Hypothesis Testing Hypothesis 1 <p> When participation expectations were confirmed, IS user attitudes and performance were expected to increase with corresponding increases in mode-of-participation. Table 3 presents the pairwise comparisons for the dependent measures. As the mode-of-participation increased from mute to instrumental voice to instrumental voice and choice, treatment means for all variables increased correspondingly with only two pairwise exceptions. Perceptions of control over the development process did not change significantly between the mute and instrumental voice conditions. Also, performance measures @ @ @ @ @ @ @ @ @ @ overall results provide partial support for H1. Hypothesis 2 <p> According to the value-expressive perspective of procedural justice, noninstrumental voice should affect perceptions of procedural justice, but control-oriented variables, such as perceived influence over the decision outcome, outcome satisfaction and task commitment, should not be influenced. From table 3, perceived procedural justice did increase significantly with the addition of noninstrumental voice, but the other attitudinal variables were unchanged. Therefore, H2 is fully supported. Hypothesis 3 <p> When participation expectations were violated, increasing levels of participation were posited to result in corresponding decreases in IS user attitudes and performance. Pairwise comparisons for the dependent measures are shown on table 3. As the mode-of-participation increased from mute to instrumental voice to instrumental voice and choice, and expectations of participation were violated, treatment means for all variables decreased correspondingly. These results indicate that H3 is fully supported. Hypothesis 4 <p> This hypothesis posited that perceptions of procedural justice would significantly decrease when noninstrumental voice was promised and that promise was subsequently violated. The @ @ @ @ @ @ @ @ @ @ 3 shows that procedural justice perceptions did decrease significantly while all other dependent measures were unaffected. Measures of performance could not be affected because the violation of noninstrumental voice occurred after the ten-minute performance period. Therefore, H4 is fully supported. Testing for Efficacy and Expectancy Effects <p> Subjects in the **25;852;TOOLONG violated condition may have been influenced unintentionally by the performance screen 's attributes( field color, background color and current date field) because the subjects did not express a preference for the attributes provided. These subjects may have felt they could not perform as well on the screen they were given as compared to the one they chose, regardless of the effort they put forth. This is an efficacy concern. Those same subjects may also have believed their ability to earn the performance-related rewards( lottery tickets) was hindered due to the performance screen 's attributes. This is an expectancy concern. <p> To test for these possible effects, subjects responded to efficacy and expectancy questions about the field color, background color and placement of @ @ @ @ @ @ @ @ @ @ Dual questions were asked to obtain an estimate of reliability and related questions were averaged to form single indices. From ANOVA and Duncan 's Multiple Range tests( alpha =.05), no significant differences between treatment conditions were detected, thereby ruling out the likelihood of systematic influences due to perceptions of efficacy and expectancy. Table 4 presents the results of efficacy and expectancy testing. V. DISCUSSION <p> In this study, both attitudes and behaviors were influenced by manipulating two independent variables :( 1) mode-of-participation, and( 2) **25;879;TOOLONG. An interaction effect was found when expectations of participation were either confirmed or violated. The findings are in accord with both the control-oriented and value-expressive theories of procedural justice. Study findings enhance prior research in user participation and procedural justice. <p> Three major findings were obtained in this study. First, IS user attitudes and performance generally paralleled perceptions of procedural justice and control when expectations of participation were confirmed. The addition of choice to instrumental voice was perceived as a stronger form of participation @ @ @ @ @ @ @ @ @ @ the predictions of the control-oriented theories of procedural justice with only two exceptions : neither perceptions of control nor performance were significantly different between the mute and the instrumental voice condition when expectations of participation were confirmed. This may be due to the relatively minor cosmetic changes allowed( meaning the background color, field color and placement of the current date field) combined with the relatively weak nature of the instrumental voice manipulation. <p> The second major finding arises from examining the dysfunctional consequences of participation when expectations of participation were violated. When subjects were promised increasingly stronger levels of participation and those promises subsequently were violated, perceptions of procedural justice, control, IS satisfaction, task commitment and actual performance decreased monotonically. These results support the control-oriented theory of procedural justice( e.g., a violation of increasing participation levels resulted in corresponding decreases in control perceptions), although this theory does not explicitly consider the issue of violating participation expectations. This aspect of participation has seldom been studied, and these results add additional evidence to the frustration @ @ @ @ @ @ @ @ @ @ Baldwin et al. 1991 ; Folger 1977 ; Cohen 1985). <p> It is interesting to contrast the effect of instrumental voice when expectations of participation either were confirmed or violated. There were no perceived control or performance differences between the mute and instrumental-voice-only conditions when expectations were confirmed. When expectations were violated, significant differences in both dependent measures were observed. It seems that when something is promised and then taken away, it may have greater intrinsic value to the recipient than were it not promised. Perhaps any violation of expectations evokes psychological discomfort beyond just the decision at hand. <p> The third major finding of this study concerns the effect of noninstrumental voice. When noninstrumental voice was added to instrumental voice and choice, perceptions of procedural justice increased( decreased) with a corresponding confirmation( violation) of participation expectations while perceived control, IS satisfaction and task commitment were unchanged. These findings are consistent with the value-expressive theory of procedural justice in that confirmation( violation) of noninstrumental voice may increase( decrease) @ @ @ @ @ @ @ @ @ @ outcome is known and unaffected by the expression of noninstrumental voice. <p> In this study, salient aspects of the decision outcome( e.g., performance-screen) were held constant across all treatment conditions. Yet, perceptions of procedural justice and control seemed to have significantly affected IS user satisfaction and performance. This indicates that the effects realized in this study were primarily psychological in nature. These results contribute to both the procedural justice and IS literature for they point to the importance of the development process in light of constant outcomes. <p> It is interesting to note that the changes made to the performance screen were cosmetic. No functional improvements were incorporated. Even so, the manipulations evoked strong attitudinal and performance reactions. Had the allowed changes been more functional in nature, it is intuitive to believe that reactions may have been more pronounced. <p> Procedural justice theories may help to clarify the psychological processes at work when affected parties are expected to become involved in decision making. The theoretical framework of procedural justice may provide a means @ @ @ @ @ @ @ @ @ @ involvement( an internal psychological state), as described by Barki and Hartwick( 1994). When studying the effect of participation on attitudes and behaviors, researchers might consider assessing the subjects ' perceived congruence, or lack thereof, between expected participation and actual participation. A congruence deficit, where expected participation exceeds perceived actual participation, may be an unintended moderating factor influencing the relationship between participation and consequences. This moderator could partially account for the mixed results reported in past participation studies. <p> The good news for IS developers and managers is that a judiciously chosen participation strategy implemented via instrumental voice, choice, and noninstrumental voice may enhance salient IS user attitudes and behavior when the solicitation for participation is genuine. A particularly strong finding of this study is that objective measures of performance, a variable of great interest to practitioners, increased about 24 percent from the mute to the instrumental voice and choice condition when participation expectations were confirmed. <p> An equally strong finding of this study is that disingenuous solicitation for participation resulted @ @ @ @ @ @ @ @ @ @ mute to the instrumental voice and choice condition. Therefore, if a participation strategy is implemented just to appease the IS user and the decision maker is merely providing the illusion of sincerity, the better strategy may be to make the decision without soliciting participation. Limitations <p> The student subjects in this study may not be reflective of IS users in the workplace. However, the purpose of this research was to test the robust predictions of procedural justice theories in the context of developing software. There is no reason to believe the underlying psychological processes reflected in these findings will operate differently in the work place. Procedural justice researchers who have examined this issue of process fairness in a wide range of contexts( e.g., with full-time employees) have documented similar results, providing additional evidence supporting the generality of the voice and choice constructs( Alexander and Ruderman 1987 : Greenberg 1986 ; Lissak et al. 1983 ; Lissak 1983). Also, the immediate reactive effects, both positive and negative, found in this study may @ @ @ @ @ @ @ @ @ @ Developing an information system traditionally takes place in five stages : planning, analysis, design, implementation and post-implementation. Because of the sequential nature of these stages, the participation strategy in one stage can influence participation strategies employed in subsequent stages. 3 It might be useful to examine the interactive effects of participation strategies between these stages. For example, participation at a later stage might negate the lack of voice and choice in an earlier stage. Also, the relative effect of voice and choice in an earlier stage might be more influential than in a later stage. <p> Overall, it seems as though procedural justice research may offer a firm theoretical basis for examining user participation. While Mumford et al.( 1983), Mumford( 1981), Mumford and Henshall( 1979) and Lawrence and Low( 1993) recognized the importance of control and representativeness in user participation, procedural justice theories provide more definitive means of operationalizing and understanding these and other psychological process variables. Color-blind individuals were asked not to participate to @ @ @ @ @ @ @ @ @ @ differ in no known respects( e.g., intellectual or emotional) from those who are not colorblind( Miller 1978). Therefore, the subject sample should not be biased in any meaningful way. A pilot study was conducted to examine whether placement of the current date field affected subjects ' satisfaction with the payroll screen or their performance level. No differences were found on either measure whether the current date field was placed on the top left, top right or top center of the payroll screen. Not all IS development methods are sequential in nature. For example, prototyping is a development methodology that is both iterative and recursive in nature. However, where sequential development methods are used, the effect of voice and choice within and between the development stages might be examined. TABLE 1 Results of ANOVA/ANCOVA Testing PREFORMATTED TABLE TABLE 2 Correlations, Means and Standard Deviations PREFORMATTED TABLE <p> a All correlations are significant at p <.01. <p> b Performance mean and standard deviation are adjusted for the effect of covariates. TABLE @ @ @ @ @ @ @ @ @ @ each dependent measure, means with different superscripts are significantly different at the alpha =.05 level using Duncan 's Multiple Range pairwise comparison test. For example, for procedural justice with expectations confirmed, 4.67, 5.79, 6.85 and 7.79 are all significantly different from one another. <p> b Expectations in the mute condition were neither confirmed nor violated. However, they were included in both the confirmed and violated ANOVA/ANCOVA models as a control. <p> c Performance means are adjusted for the effect of covariates( i.e., they represent least square means). TABLE 4 Testing for Efficacy and Expectancy Concerns PREFORMATTED TABLE Efficacy Items <p> 1. To what extent did the( background color, field color, placement of the current date field) on your performance screen influence the amount of payroll data you entered during the ten-minute performance period? <p>( 1 = Strong negative influence, 5 = No influence, 9 = Strong positive influence) <p> To what degree did the( background color, field color, placement of the current @ @ @ @ @ @ @ @ @ @ to enter payroll data during the ten-minute performance period? <p>( 1 = Strong negative affect, 5 = No affect, 9 = Strong positive affect) Expectancy Items <p> 1. To what extent did the( background color, field color, placement of the current date field) on your performance screen influence the number of lottery tickets you expected you could earn? <p>( 1 = Strong negative influence, 5 = No influence, 9 = Strong positive influence) <p> To what degree did the( background color, field color, placement of the current date field) on your performance screen affect the number of lottery tickets you expected you could earn? <p>( 1 = Strong negative effect, 5 = No affect, 9 = Strong positive affect) <p> a Inter-item correlation estimate of reliability. <p> DIAGRAM : FIGURE 1 Hypothesized Relations and Effects APPENDIX A Dependent Variable, Manipulation Check and General Post-Test Measure Panel A : Dependent Variable Measures Procedural Justice : How fair were the procedures used by the experimenter in @ @ @ @ @ @ @ @ @ @ data entry screen you used? = Not at all Fair, 5 = Neither Fair nor Unfair, 9 = Extremely Fair Procedural Justice : How just was the way in which the experimenter decided what changes to make to your PERFORMANCE-SCREEN( meaning the second payroll data entry screen you used)? = Not at all Just, 5 = Neither Just nor Unjust, 9 = Extremely Just Control : How much influence did you have in determining which changes would be made to your PERFORMANCE-SCREEN( meaning the second payroll data entry screen you used)? = No Influence, 5 = Moderate Influence, 9 = Complete Influence Control : How much control did you have in determining how your PERFORMANCE-SCREEN looked( meaning the second payroll data entry screen you used)? = No Control, 5 = Moderate Control, 9 = Complete Control Satisfaction : Overall, I am satisfied with the second payroll data entry screen( meaning the PERFORMANCE-SCREEN). = Strongly Disagree, 5 = Neither Agree nor Disagree, 9 = Strongly Agree Satisfaction @ @ @ @ @ @ @ @ @ @ entry screen( meaning the PERFORMANCE-SCREEN)? = Very dissatisfied, 5 = Neither Satisfied nor Dissatisfied, 9 = Very Satisfied Task Commitment : How committed were you to working on the task of entering the time card data into the computer? = Not at all Committed, 5 = Neither Committed nor Uncommitted, 9 = Fully Committed Task Commitment : To what extent did you accept the task you worked on during the experiment? = Completely Rejected, 5 = Neither Rejected nor Accepted, 9 = Completely Accepted Panel B : Manipulation Check Measures Instrumental Voice : The experimenter promised me that, after the five-minute training period, I would get to express my opinion about which screen attributes I preferred to see in the PERFORMANCE-SCREEN( meaning the second payroll screen). 1 = Strongly Disagree, 5 = Neither Agree nor Disagree, 9 = Strongly Agree Instrumental Voice : After the training period, I told the experimenter which screen attributes preferred to see in the PERFORMANCE-SCREEN( meaning the second payroll screen). 1 = Strongly @ @ @ @ @ @ @ @ @ @ = Strongly Agree Choice : The experimenter promised me that, before the ten-minute performance period, I would get to choose one combination of screen attributes, from among three different options, to incorporate into the PERFORMANCE-SCREEN( meaning the second payroll screen). = Strongly Disagree, 5 = Neither Agree nor Disagree, 9 = Strongly Agree Choice : Before the ten-minute performance period, I was asked to choose one combination of screen attributes, from among three different options, to incorporate into the PERFORMANCE-SCREEN( meaning the second payroll screen). = Strongly Disagree, 5 = Neither Agree nor Disagree, 9 = Strongly Agree Noninstrumental Voice : The experimenter promised me that, after the ten-minute performance period, I would get to provide additional recommendations about how I might further improve the PERFORMANCE-SCREEN( meaning the second payroll screen). = Strongly Disagree, 5 = Neither Agree nor Disagree, 9 = Strongly Agree Noninstrumental Voice : After the ten-minute performance period, the experimenter asked me for some recommendations about how to further improve the @ @ @ @ @ @ @ @ @ @ Strongly Disagree, 5 = Neither Agree nor Disagree, 9 = Strongly Agree Panel C : General Post-test Measures Please enter your age -- Please indicate your gender -- This question is attempting to determine your level of general experience using computers. This questions concerns you knowledge of general software, such as word processors and spreadsheets. This question does not ask if you can program computers and other such technical tasks. Please select a number at the point where you consider yourself to be. <p> My degree of computer experience is : = Low, 5 = Moderate, 9 = High How understandable were the instructions to the experiment? = Not Understandable, 5 = Somewhat Understandable, 9 = Very Understandable You are receiving course credit for participating in this experiment. Are you satisfied with the amount of course credit you are receiving, given the amount of effort you have put into this experiment? = Not Very Satisfied, 5 = Moderately Satisfied, 9 = Very Satisfied <p> The amount of lottery tickets I earned was determined @ @ @ @ @ @ @ @ @ @, meaning, the better I performed, the more lottery tickets I would earn. = Strongly Disagree, 5 = Neither Agree nor Disagree, 9 = Strongly Agree I talked to other participants during this experiment. = Strongly Disagree, 5 = Neither Agree nor Disagree, 9 = Strongly Agree I am frustrated with the experimenter. = Strongly Disagree, 5 = Neither Agree nor Disagree, 9 = Strongly Agree I discussed the nature of this experiment with others prior to my participation. = Strongly Disagree, 5 = Neither Agree nor Disagree, 9 = Strongly Agree What do you think this experiment is about? Please write any additional comments you have about this experiment. <p> a12. I am frustrated that the performance-screen did not include the attributes I wanted. <p> 1 = Strongly Disagree, 5 = Neither Agree nor Disagree, 9 = Strongly Agree <p> a13. Please explain why you believe your preferences and/or choices were not incorprated into the performance-screen( meaning the second payroll data entry screen you used). <p> @ @ @ @ @ @ @ @ @ @ subjects. <p> 
