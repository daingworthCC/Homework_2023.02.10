5184041 <h> " Real data can be a pain " <p> Some time ago, I McLaughlin was handed a dataset that needed to be modeled. It was generated as follows : <p> 1. Random navigation errors, historically a binary mixture of normal and Laplace with a common mean, were collected by observation. <p> 2. Sadly, these data were recorded with too few decimal places so that the resulting quantization is clearly visible in a scatterplot. <p> 3. The quantized data were then interpolated( to an unobserved location). <p> The final result looks like fuzzy points( small scale jitter) at quantized intervals spanning a much larger scale( the parent mixture distribution). This fuzziness, likely normal or Laplace, results from the interpolation. Otherwise, the data would look like a discrete analogue of the normal/Laplace mixture. <p> I would like to characterize the latent normal/Laplace mixture distribution but the quantization is " getting in the way ". When I tried MCMC on this problem( using JAGS) and simply ignoring the quantization, it turned out that the @ @ @ @ @ @ @ @ @ @ to the underlying parent mixture so that the MCMC process did not " see " the normal/Laplace distribution very well. That is, it returned unrealistic parameters for it, with values more in tune with the jitter than with the parent distribution. In other words, it thought it was modeling the jitter. <p> So I can not ignore the quantization. Unfortunately, I have not found any way to incorporate it into a valid MCMC model either. I 've searched everywhere I can find but have uncovered no examples for treating quantization error corrupting a parent distribution. <p> Do you, or your readers, have any suggestions? <p> My reply : I think this should work fine in Stan( or any other Bayesian software) if you just model the steps 1, 2, 3 directly : <p> For step 1, you have your mixture model. You just need to put an informative prior distribution on the parameters of the mixture components to get a stable estimate. <p> For step 2, just model the @ @ @ @ @ @ @ @ @ @, and the rounded data are yi, then set up a model for yz, maybe just a simple rounding model. The z 's are now missing data. Or in this case( a mixture of normal and laplace densities for z), you should be able to simply integrate out the missingness to get a probability distribution for the rounded data, z. <p> For step 3, I do n't quite know what you mean by " interpolated to an unobserved location. " Again, though, this is just some process that can be modeled. <p> The moral of the story is : likelihood inference really works! The trick is to model the data and then do the inference, rather than trying to jump directly to create an estimate from the data. <h> 4 Comments <p> Generate parameters from the prior, generate the data and then condition on appropriately quantized and interpolated to location data. <p> The target posterior is the correct one for that data process and you could get a simulation approximation for @ @ @ @ @ @ @ @ @ @ these up into fully accurate data, quantized data and interpolated to location.) <p> Doing it analytically is much better and likely the only feasible way for the full data set, but doing the above for just a small subset might help clarify and guide doing it analytically. <p> I 'll keep this paper as an example that you can publish a 15+ page journal paper about absolutely nothing -- worrying about measure zero in applications ; -) <p> I do n't think Cox and Hinkley spent more than a paragraph or two adequately dealing with it -- though, as we know, even really smart people forget about it. <p> Models are representations of what is being represented -- what is being represented does not immediately/always inherit the properties of the representation. Here probability generating models represent observations and continuos ones are very, very convenient but the observations can not be really continuous with infinite precision. <p> Now it mat not be very convenient when the continuous assumptions cause problems and need to be fixed up 
